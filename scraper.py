import requests
from bs4 import BeautifulSoup, Tag
import time
import re
import random # å¯¼å…¥randomåº“
from typing import List, Dict, Any, Optional, Set

from data_models import Player
from utils import get_continent_from_nationality

class LiquipediaScraper:
    def __init__(self, config: Dict[str, Any]):
        self.base_url = config['BASE_URL']
        # self.delay ä¸å†æ˜¯ä¸»è¦å»¶è¿Ÿæ‰‹æ®µï¼Œä½†å¯ä½œä¸ºåŸºç¡€å€¼
        self.delay = config['REQUEST_DELAY_SECONDS']
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': config['USER_AGENT']})
        self.processed_player_urls: Set[str] = set()

    def _get_soup(self, url: str, max_retries: int = 3) -> Optional[BeautifulSoup]:
        """
        ä½¿ç”¨requestså‘èµ·è¯·æ±‚å¹¶è¿”å›žSoupå¯¹è±¡ã€‚
        å†…ç½®äº†éšæœºå»¶è¿Ÿã€æŒ‡æ•°é€€é¿å’Œè‡ªåŠ¨é‡è¯•æœºåˆ¶ã€‚
        """
        full_url = self.base_url + url
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
                # åŸºç¡€å»¶è¿Ÿ + 0åˆ°2ç§’çš„éšæœºé¢å¤–å»¶è¿Ÿ
                sleep_time = self.delay + random.uniform(0, 2)
                time.sleep(sleep_time)

                print(f"   - æ­£åœ¨è¯·æ±‚ (å°è¯• {attempt + 1}/{max_retries}): {full_url}")
                response = self.session.get(full_url, timeout=20)
                
                # å¦‚æžœæ˜¯429é”™è¯¯ï¼Œåˆ™è¿›è¡ŒæŒ‡æ•°é€€é¿
                if response.status_code == 429:
                    # ä»Žå“åº”å¤´èŽ·å–å»ºè®®çš„ç­‰å¾…æ—¶é—´ï¼Œå¦‚æžœæ²¡æœ‰åˆ™è‡ªå·±è®¡ç®—
                    retry_after = int(response.headers.get("Retry-After", 30 * (attempt + 1)))
                    print(f"   - âš ï¸ æ”¶åˆ° 429 é€ŸçŽ‡é™åˆ¶è­¦å‘Šã€‚å°†é€€é¿ {retry_after} ç§’...")
                    time.sleep(retry_after)
                    last_exception = requests.RequestException(f"Gave up after 429, attempt {attempt + 1}")
                    continue # ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªçŽ¯å°è¯•

                response.raise_for_status() # å¯¹å…¶ä»–é”™è¯¯(å¦‚404, 500)åˆ™ç›´æŽ¥æŠ›å‡ºå¼‚å¸¸
                return BeautifulSoup(response.text, 'lxml')

            except requests.RequestException as e:
                last_exception = e
                print(f"   - âŒ è¯·æ±‚ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                # å¯¹äºŽéž429çš„è¿žæŽ¥é”™è¯¯ï¼Œä¹Ÿè¿›è¡ŒçŸ­æš‚ç­‰å¾…åŽé‡è¯•
                time.sleep(5 * (attempt + 1))
                continue
        
        print(f"   - âŒ åœ¨ {max_retries} æ¬¡å°è¯•åŽï¼Œè¯·æ±‚å½»åº•å¤±è´¥: {full_url}")
        print(f"   - æœ€ç»ˆé”™è¯¯: {last_exception}")
        return None

    def _get_infobox_value(self, infobox: Tag, key: str) -> Optional[str]:
        cell = infobox.find('div', class_='infobox-cell-1', string=re.compile(f'\\b{key}\\b', re.I))
        if cell and cell.find_next_sibling('div', class_='infobox-cell-2'):
            return cell.find_next_sibling('div').text.strip()
        return None

    def _parse_player_profile(self, player_url: str) -> Optional[Player]:
        print(f"   - æ­£åœ¨è§£æžé€‰æ‰‹: {player_url}")
        soup = self._get_soup(player_url)
        if not soup: return None
        name = soup.find('h1', class_='firstHeading').text.strip()
        infobox = soup.find('div', class_='infobox-cs')
        if not infobox: return None
        try:
            born_text = self._get_infobox_value(infobox, 'Born') or ''
            age_match = re.search(r'\(age\s+(\d+)\)', born_text)
            age = int(age_match.group(1)) if age_match else 0
            nationality = self._get_infobox_value(infobox, 'Nationality') or 'N/A'
            role = self._get_infobox_value(infobox, 'Role') or 'Rifler'
            club = self._get_infobox_value(infobox, 'Team')
            major_count = 0
            results_table = soup.find('table', class_='wikitable-striped')
            if results_table:
                rows = results_table.find_all('tr')
                for row in rows:
                    if row.find('th'): continue
                    tier_cell = row.select_one('td:nth-of-type(1) a')
                    event_cell = row.select_one('td:nth-of-type(4)')
                    if tier_cell and event_cell:
                        tier = tier_cell.get('title', '').lower()
                        event_name = event_cell.text.lower()
                        if 's-tier' in tier and 'major' in event_name:
                            major_count += 1
            return Player(name=name, age=age, role=role, nationality=nationality, continent=get_continent_from_nationality(nationality), club=club, major_participations=major_count)
        except Exception as e:
            print(f"   - âŒ è§£æžé€‰æ‰‹é¡µé¢HTMLå¤±è´¥: {player_url} - {e}")
            return None

    def _scrape_player_category(self, category_url: str, limit: int = 50) -> List[Player]:
        players = []
        next_page_url = category_url
        while next_page_url and len(players) < limit:
            soup = self._get_soup(next_page_url)
            if not soup: break
            player_group = soup.find('div', class_='mw-category-group')
            if not player_group: break
            for player_tag in player_group.select('ul > li > a'):
                player_url = player_tag['href']
                if player_url.startswith('/counterstrike/index.php?title='): continue
                if player_url in self.processed_player_urls: continue
                player_data = self._parse_player_profile(player_url)
                if player_data and player_data.major_participations > 0:
                    players.append(player_data)
                self.processed_player_urls.add(player_url)
                if len(players) >= limit: break
            pagination_links = soup.select('div.mw-category-generated a')
            next_page_url = None
            for link in pagination_links:
                if 'next page' in link.text:
                    next_page_url = link['href']
                    break
        return players

    def get_retired_legends(self) -> List[Player]:
        print("\nðŸ” å¼€å§‹èŽ·å–ã€å·²é€€å½¹ã€‘çš„Majorå…«å¼ºé€‰æ‰‹...")
        category_url = '/counterstrike/index.php?title=Category:Retired_Players'
        players = self._scrape_player_category(category_url)
        print(f"   âœ”ï¸ æ‰¾åˆ° {len(players)} åç¬¦åˆæ¡ä»¶çš„é€€å½¹é€‰æ‰‹ã€‚")
        return players

    def get_active_major_players(self) -> List[Player]:
        print("\nðŸ” å¼€å§‹èŽ·å–ã€çŽ°å½¹ã€‘çš„Majorå‚èµ›é€‰æ‰‹...")
        category_url = '/counterstrike/index.php?title=Category:Active_Players'
        players = self._scrape_player_category(category_url)
        print(f"   âœ”ï¸ æ‰¾åˆ° {len(players)} åç¬¦åˆæ¡ä»¶çš„çŽ°å½¹é€‰æ‰‹ã€‚")
        return players

    def fetch_all_players(self) -> List[Player]:
        all_players = []
        all_players.extend(self.get_retired_legends())
        all_players.extend(self.get_active_major_players())
        return all_players