import requests
from bs4 import BeautifulSoup, Tag
import time
import re
from typing import List, Dict, Any, Optional, Set

from data_models import Player
from utils import get_continent_from_nationality

class HLTVScraper:
    """
    å°è£…äº†æ‰€æœ‰ä»HLTV.orgçˆ¬å–å’Œè§£ææ•°æ®çš„é€»è¾‘ã€‚
    """
    def __init__(self, config: Dict[str, Any]):
        self.base_url = config['BASE_URL']
        self.delay = config['REQUEST_DELAY_SECONDS']
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': config['USER_AGENT']})
        self.processed_player_urls: Set[str] = set()

    def _get_soup(self, url: str) -> Optional[BeautifulSoup]:
        """
        å‘èµ·è¯·æ±‚å¹¶è¿”å›BeautifulSoupå¯¹è±¡ï¼ŒåŒ…å«å»¶è¿Ÿå’Œé”™è¯¯å¤„ç†ã€‚
        """
        # ç¡®ä¿æ˜¯å®Œæ•´çš„URL
        full_url = url if url.startswith('http') else self.base_url + url
        try:
            time.sleep(self.delay)
            response = self.session.get(full_url, timeout=15)
            response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸
            return BeautifulSoup(response.text, 'lxml')
        except requests.RequestException as e:
            print(f"âŒ è¯·æ±‚é”™è¯¯: {full_url} - {e}")
            return None

    def _parse_player_profile(self, player_url: str) -> Optional[Player]:
        """
        è§£æå•ä¸ªé€‰æ‰‹é¡µé¢ï¼Œæå–æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯ã€‚
        """
        print(f"   - æ­£åœ¨è§£æé€‰æ‰‹: {player_url}")
        soup = self._get_soup(player_url)
        if not soup:
            return None

        try:
            # CSSé€‰æ‹©å™¨æ˜¯åŸºäºå½“å‰HLTVé¡µé¢ç»“æ„
            name = soup.find('h1', class_='player-headline').text.strip()
            age_text = soup.find('div', text='Age').find_next_sibling('div').text.strip()
            age = int(age_text.split()[0]) # '24 years old' -> '24'
            
            nationality_tag = soup.find('div', text='Nationality').find_next_sibling('div').find('img')
            nationality = nationality_tag['title'] if nationality_tag else 'N/A'
            
            team_tag = soup.find('div', text='Team').find_next_sibling('div').find('a')
            club = team_tag.text.strip() if team_tag else None
            
            # è§’è‰²ä¿¡æ¯å¯èƒ½ä¸å­˜åœ¨ï¼Œéœ€è¦å¥å£®å¤„ç†
            role_tag = soup.find('div', class_='player-stat-row', text='Primary role')
            role = role_tag.find_next_sibling('div').text.strip() if role_tag else 'Rifler' # é»˜è®¤ä¸ºRifler

            # --- ç®€åŒ–é€»è¾‘è­¦å‘Š ---
            # å®Œæ•´è·å–Majorå‚ä¸æ¬¡æ•°å’Œå…«å¼ºè®°å½•éœ€è¦æ·±åº¦éå†é€‰æ‰‹çš„æ‰€æœ‰èµ›äº‹é¡µé¢ï¼Œ
            # è¿™ä¼šæå¤§å¢åŠ çˆ¬è™«çš„å¤æ‚åº¦å’Œè¿è¡Œæ—¶é—´ã€‚
            # æ­¤å¤„é‡‡ç”¨ç®€åŒ–é€»è¾‘ï¼šä»é€‰æ‰‹çš„â€œæˆå°±â€æ ä¸­æŸ¥æ‰¾Majorå¥–æ¯æ•°é‡ä½œä¸ºæ›¿ä»£ã€‚
            # è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼ï¼Œä½†èƒ½ä¿è¯ç¨‹åºé«˜æ•ˆè¿è¡Œã€‚
            achievements = soup.find_all('div', class_='trophy-event-name', text=re.compile('Major', re.IGNORECASE))
            major_participations = len(achievements)

            # æ£€æŸ¥æ˜¯å¦æœ‰å…«å¼ºæˆ–æ›´å¥½çš„æˆç»© (MVP, Champion, Finalist, Semi-finalist)
            is_legend = False
            if major_participations > 0:
                for achievement in achievements:
                    parent_trophy = achievement.find_parent('div', class_='trophy')
                    if parent_trophy:
                        # æ£€æŸ¥å¥–æ¯å›¾ç‰‡URLæ˜¯å¦åŒ…å«1st, 2nd, 3-4th æˆ– mvp
                        img_src = parent_trophy.find('img')['src']
                        if any(s in img_src for s in ['/1st', '/2nd', '/3-4th', '/mvp']):
                            is_legend = True
                            break
            # å¯¹äºæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬ç›´æ¥è¿”å›è§£æå‡ºçš„ä¿¡æ¯ï¼Œç”±è°ƒç”¨è€…åˆ¤æ–­æ˜¯å¦ç¬¦åˆæ¡ä»¶
            
            return Player(
                name=name,
                age=age,
                role=role,
                nationality=nationality,
                continent=get_continent_from_nationality(nationality),
                club=club,
                major_participations=major_participations,
            )
        except (AttributeError, IndexError, ValueError) as e:
            print(f"   - âŒ è§£æé€‰æ‰‹é¡µé¢å¤±è´¥: {player_url} - {e}")
            return None

    def get_retired_legends(self) -> List[Player]:
        """
        è·å–æ‰€æœ‰è¢«HLTVæ ‡è®°ä¸ºâ€œé€€å½¹â€ä¸”æ›¾è¿›å…¥Majorå…«å¼ºçš„é€‰æ‰‹ã€‚
        """
        print("\nğŸ” å¼€å§‹è·å–ã€å·²é€€å½¹ã€‘çš„Majorå…«å¼ºé€‰æ‰‹...")
        # HLTVçš„æœç´¢åŠŸèƒ½å¯ä»¥ç­›é€‰é€€å½¹é€‰æ‰‹
        # æ³¨æ„ï¼šHLTVçš„æœç´¢ç»“æœé¡µé¢å¯èƒ½æ˜¯åŠ¨æ€åŠ è½½çš„ï¼Œè¿™é‡Œåªæ¼”ç¤ºçˆ¬å–ç¬¬ä¸€é¡µ
        search_url = self.base_url + '/results?playerFilter=retired'
        soup = self._get_soup(search_url)
        if not soup:
            return []
        
        retired_players = []
        player_tags = soup.select('td.player-world-rank a') # æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
        
        if not player_tags: # å¤‡ç”¨é€‰æ‹©å™¨
             player_tags = soup.select('div.result-player a')

        for tag in player_tags[:30]: # é™åˆ¶æ•°é‡ä»¥åŠ å¿«æ¼”ç¤º
            player_url = tag['href']
            if player_url in self.processed_player_urls:
                continue
            
            player_data = self._parse_player_profile(player_url)
            if player_data and player_data.major_participations > 0:
                # æ£€æŸ¥æ˜¯å¦æœ‰å…«å¼ºæˆç»©ï¼ˆåŸºäºç®€åŒ–çš„é€»è¾‘ï¼‰
                # ä¸ºäº†ç¬¦åˆéœ€æ±‚ï¼Œæˆ‘ä»¬éœ€è¦ç¡®è®¤é€‰æ‰‹çœŸçš„è¿›è¿‡å…«å¼ºï¼Œè¿™é‡Œçš„ç®€åŒ–é€»è¾‘å¯èƒ½ä¸å¤Ÿç²¾ç¡®
                # ä½†æ ¹æ®æˆ‘ä»¬çš„ç®€åŒ–é€»è¾‘ï¼Œåªè¦æœ‰Majorå¥–æ¯å°±ç®—å‚ä¸è¿‡
                # è¿™é‡Œæˆ‘ä»¬å‡è®¾åªè¦å‚ä¸è¿‡Majorçš„é€€å½¹é€‰æ‰‹ï¼Œæˆ‘ä»¬éƒ½æ£€æŸ¥ï¼ˆå®é™…éœ€æ±‚æ˜¯æ£€æŸ¥å…«å¼ºï¼‰
                 retired_players.append(player_data)

            self.processed_player_urls.add(player_url)
        print(f"   âœ”ï¸ æ‰¾åˆ° {len(retired_players)} åç¬¦åˆæ¡ä»¶çš„é€€å½¹é€‰æ‰‹ã€‚")
        return retired_players


    def get_active_major_players(self) -> List[Player]:
        """
        è·å–æ‰€æœ‰å‚åŠ è¿‡Majorçš„ç°å½¹é€‰æ‰‹ã€‚
        æœ€ä½³ç­–ç•¥æ˜¯ä»æˆ˜é˜Ÿæ’åå…¥æ‰‹ã€‚
        """
        print("\nğŸ” å¼€å§‹è·å–ã€ç°å½¹ã€‘çš„Majorå‚èµ›é€‰æ‰‹...")
        ranking_url = self.base_url + '/ranking/teams'
        soup = self._get_soup(ranking_url)
        if not soup:
            return []

        active_players = []
        team_tags = soup.select('div.ranked-team a.team-name') # åŒæ ·ï¼Œé€‰æ‹©å™¨å¯èƒ½å˜åŒ–

        for team_tag in team_tags[:20]: # é™åˆ¶åœ¨Top 20æˆ˜é˜Ÿä»¥æé«˜æ•ˆç‡
            team_url = team_tag['href'].replace('/team/', '/sendredirect/team/')
            team_soup = self._get_soup(team_url)
            if not team_soup:
                continue
            
            player_tags = team_soup.select('td.player-holder a')
            for player_tag in player_tags:
                player_url = player_tag['href']
                if player_url in self.processed_player_urls:
                    continue
                
                player_data = self._parse_player_profile(player_url)
                if player_data and player_data.major_participations > 0:
                    active_players.append(player_data)

                self.processed_player_urls.add(player_url)
        print(f"   âœ”ï¸ æ‰¾åˆ° {len(active_players)} åç¬¦åˆæ¡ä»¶çš„ç°å½¹é€‰æ‰‹ã€‚")
        return active_players

    def fetch_all_players(self) -> List[Player]:
        """
        æ‰§è¡Œæ‰€æœ‰çˆ¬å–ä»»åŠ¡å¹¶è¿”å›åˆå¹¶åçš„é€‰æ‰‹åˆ—è¡¨ã€‚
        """
        all_players = []
        all_players.extend(self.get_retired_legends())
        all_players.extend(self.get_active_major_players())
        return all_players